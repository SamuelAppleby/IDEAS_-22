\begin{figure}[!t]
	\hspace*{-3mm}
	\includegraphics[width=1.05\linewidth]{images/sqlminer_benchmark.pdf}
	\caption{KnoBAB vs SQLMiner Performance for 25 clauses with frequent activity labels with Support and Trace Information. OOM indicates Out of Secondary Memory for logs containing $10^3$ traces (followed by the time taken for an exception to occur).}\label{fig:vsSQL}
\end{figure}
\section{Experimental Analysis}\label{sec:exp}
Our benchmarks exploited a Razer Blade Pro on Ubuntu 20.04: Intel Core i7-10875H CPU @ 2.30GHz - 5.10 GHz, 16GB DDR4 2933MHz RAM, 180GB free disk space. Our datasets (\tablename~\ref{table:dataset}) include 2 real life event logs: BPIC 2011 (Dutch academic hospital log) and BPIC 2012 (Dutch loan company) from the Business Processing Intelligence Challenge.

\begin{table}
	\resizebox{.5\textwidth}{!}{
			\begin{tabular}{ l l r r r }
			\toprule
			Benchmark & Datasets & Traces & Events & Unique Activities\\ [0.5ex] 
			\midrule
			\multirow{4}{*}{SQL Miner} &
			bpic\_2011\_original & 1143 & 150 291 & 624 \\ &
			bpic\_2011\_10 & 10 & 2613 & 26 \\ &
			bpic\_2011\_100 & 100 & 12 195 & 26 \\ &
			bpic\_2011\_1000 & 1000 & 133 935 & 28 \\ 
			\midrule
			\multirow{1}{*}{Declare Analyzer} &
			bpic\_2012\_original & 13087 & 262 200 & 21 \\
			\bottomrule
		\end{tabular}
	}
\caption{Range of datasets used for benchmarking\tablefootnote{\url{https://dx.doi.org/10.17605/OSF.IO/XWD3V}}.}
\label{table:dataset}
\end{table}

\begin{figure*}[!h]
	\begin{minipage}[t]{.3\linewidth}
		\vspace*{-7cm}
		\centering
		\includegraphics[width=\textwidth]{images/burattin_benchmark2.pdf}
		\captionof{figure}{KnoBAB vs Declare Analyzer Performance for Different Models. %Knobab + CQ indicates the intersection, per trace, of all the satisfaction result representations i.e. returning  the traces that fulfilled all the clauses.
		}\label{fig:vsBuatto}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.66\textwidth}
		\hspace*{-.7cm}
		\centering
		\resizebox{\textwidth}{!}{
			\begin{tabular}{ll}
				Model & Queries \\
				\toprule
				\multirow{5}{*}{$M_1=\qquad$} \rdelim\{{5}{3mm} & $q_{1}:= \DeclareClause{Response}{A\_SUBMITTED}{\textbf{true}}{A\_ACCEPTED}{\textbf{true}}$ \\ & $q_{2}:= \DeclareClause{Response}{A\_SUBMITTED}{\texttt{AMOUNT\_REQ} \geq 10^3}{A\_ACCEPTED}{\textbf{true}}$ \\&
				$q_{3}:= \DeclareClause{Response}{A\_SUBMITTED}{\texttt{AMOUNT\_REQ} < 10^3}{A\_ACCEPTED}{\textbf{true}}$ \\&
				$q_{4}:= q_{1} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\&
				$q_{5}:= q_{1} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}\neq\texttt{A\_ACCEPTED.org:resource}$ \\
				\toprule
				\multirow{5}{*}{$M_2=M_1+$}\rdelim\{{5}{3mm} & $q_{6}:= \DeclareClause{Response}{W\_Completeren aanvraag}{\textbf{true}}{W\_Valideren aanvraag}{\textbf{true}}$ \\ &
				$q_{7}:= \DeclareClause{Response}{W\_Completeren aanvraag}{\textbf{true}}{O\_CANCELLED}{\textbf{true}}$ \\&
				$q_{8}:= q_{6} \textrm{ where } \texttt{W\_Valideren aanvraag.org:resource}\neq\texttt{W\_Valideren aanvraag.org:resource}$ \\&
				$q_{9}:= \DeclareClause{Response}{W\_Valideren aanvraag}{\texttt{AMOUNT\_REQ} = 5 \cdot 10^3}{O\_CANCELLED}{\textbf{true}} $ \\&
				$q_{10}:= q_{9} \textrm{ where } \texttt{W\_Valideren aanvraag.org:resource}=\texttt{O\_CANCELLED.org:resource}$ \\
				\toprule
				\multirow{6}{*}{$M_3=M_2+$}\rdelim\{{6}{3mm} & $q_{11}:= \DeclareClause{Response}{O\_SELECTED}{\textbf{true}}{O\_CANCELLED}{\textbf{true}}$ \\&
				$q_{12}:= q_{11} \textrm{ where } \texttt{O\_SELECTED.org:resource}=\texttt{O\_CANCELLED.org:resource}$ \\&
				$q_{13}:= \DeclareClause{Response}{O\_SELECTED}{\texttt{AMOUNT\_REQ} < 8 \cdot 10^3}{O\_CANCELLED}{\textbf{true}}$ \\&
				$q_{14}:= q_{13} \textrm{ where } \texttt{O\_SELECTED.org:resource}=\texttt{O\_CANCELLED.org:resource}$ \\&
				$q_{15}:= \DeclareClause{Response}{O\_SELECTED}{\texttt{AMOUNT\_REQ} > 10^3}{O\_CANCELLED}{\textbf{true}}$ \\&
				\qquad\; $\textrm{ where } \texttt{O\_SELECTED.org:resource}\neq\texttt{O\_CANCELLED.org:resource}$\\
				\toprule
				\multirow{5}{*}{$M_4=M_3+$} \rdelim\{{5}{3mm}& $q_{16}:= \DeclareClause{Response}{A\_PARTLYSUBMITTED}{\textbf{true}}{A\_DECLINED}{\textbf{true}}$ \\&
				$q_{17}:= q_{16} \textrm{ where } \texttt{A\_PARTLYSUBMITTED.org:resource}=\texttt{A\_DECLINED.org:resource}$ \\&
				$q_{18}:= \DeclareClause{Response}{A\_PARTLYSUBMITTED}{\texttt{AMOUNT\_REQ} > 2 \cdot 10^4}{A\_DECLINED}{\textbf{true}}$ \\&
				$q_{19}:= \DeclareClause{Response}{A\_PARTLYSUBMITTED}{\texttt{AMOUNT\_REQ} > 2 \cdot 10^4}{A\_CANCELLED}{\textbf{true}}$ \\&
				$q_{20}:= q_{18} \textrm{ where } \texttt{A\_PARTLYSUBMITTED.org:resource}=\texttt{A\_DECLINED.org:resource}$ \\
		\end{tabular}}
		\captionof{table}{Declare Models with their Respective Clauses}\label{table:burattin_model}
	\end{minipage}
\end{figure*} 
\subsection{SQLMiner}\label{ssec:sqlmin}
These experiments want to test our working hypotheses for the possibility of engineering a tailored relational database architecture that can outperform process mining through conformance checking running on traditional relational databases. In the latter, no \LTLf operators are exploited but a table similar to \texttt{ActivityTable} is exploited. % are provided and non-customary tabular representation is exploited. 
Given that the SQL provided in \cite{Schonig15,SchonigRCJM16} might only return the \textsc{Support} associated with each candidate Declare clause (\texttt{SQLMiner+Support}), we provided the least possible changes to also associate each candidate clause with the set of all the traces satisfying it. This was achieved by both extending the activation condition expressed in SQL and using \texttt{array\_aggr} included in \textbf{PostgreSQL 14.2} to list such traces (\texttt{SQLMiner+TraceInfo}). For comparing the same settings in KnoBAB, we run both Max-SAT and \textsc{Support} queries with the difference that, in our case, both of these implementations will always return, per intermediate result specification, the trace information satisfying each possible model clause.
For our experiments, we exploited \remove{the same hospital log} \add{BPIC 2011} dataset from \cite{SchonigRCJM16}. To test the scalability of the solutions, we recorded the query's runtime with increasing log size: we randomly sampled the log with three sub-logs containing 10, 100 and 1000 traces, while guaranteeing that each sub-log is always a subset of the greater ones. For each sub-log, we generated 8 distinct models as benchmarked in \cite{Schonig15}. Each model consists of 25 clauses instantiating the same Declare template (\textit{elected template}) with different activation and target conditions. Those did not consider payload conditions and were only considering the most frequent activity labels appearing in the sub-log. Models of greater size than this caused an exponential increase in required secondary memory for SQLMiner (on the order of TB), justifying our approach for a sampled model. In their approach, each model was queried by running the SQL query corresponding to the \textit{elected template}, and the specific activation and target conditions from the model's clauses were distinct rows in the \textsf{Action} table.



The outcome of such experiments is represented in \figurename~\ref{fig:vsSQL}, where each plot represents the running time associated with models containing the same \textit{elected template}. In the worst-case scenario (\textsf{Response}), we exhibit similar query running times to SQLMiner. Even so, we are always providing trace information, and in the case of \textsf{Response}, altering the SQL to provide this causes over an order of magnitude increase in complexity. In the best-case scenario, we outperform SQLMiner by at most 5 orders of magnitude. This is because our query plan minimizes the access to the data queries and our computation avoided explicit computations of aggregations. This was achieved by sorting the intermediate results, and, as our operators' implementations guarantee that (intermediate) results are always sorted, counting operations are just linear scans of the intermediate result representation. Our solution never exceeded the 16GB of primary memory while, for some more complex queries (top row of \figurename~\ref{fig:vsSQL}), SQLMiner exceeded it, thus proving that our solution is also memory efficient. One of the outstanding examples is \textsf{RespExistence}, where we are greatly more efficient than SQLMiner. This is a clear indicator of the potential gains from utilising our proposed \textsf{CountTable}, summarizing the appearance of activity labels in events per trace by counting their instances. The original SQL query is required to scan the whole \texttt{Log} table (similar to our \texttt{ActivityTable}), which contained all of the trace events. We also remind the reader that the \textsf{CountTable} can be efficiently created while scanning the whole log dataset, so no super-linear overhead is added at loading time. This further validates that an adequate tabular representation twinned with \xLTLf operators extending the \LTLf specification for tabular data provides a suitable solution. Last, the running time of the Max-SAT problem and \textsc{Support} for KnoBAB exhibited similar running times, while in PostgreSQL those exhibit huge variations depending on the query-plan rewriting performed by the PostgreSQL query engine. For some elected templates, our proposed \texttt{SQLMiner+TraceInfo} formulation proved also to be more efficient than the \texttt{SQLMiner+Support} queries originally proposed by \cite{Schonig15} (which contain no trace information).




\subsection{Declare Analyzer}\label{ssec:declan}
The set of experiments on Declare Analyzer have the aim of comparing our proposed solution against a solution tailored for solving Declare Conjunctive Queries over logs running exclusively in primary memory. %In particular, we want to show the benefit of running a query plan when multiple sub-queries are shared: by increasing the size of the model by adding novel queries, we are expecting that while our solution exhibits a near constant running time, Declare Analyzer's running time should increase almost linearly as per the authors' claim.
%=======
%>>>>>>> 33357ef815c5a234b684fdafdceed45bf3a89be0
%In order to do so, 
We exploited the \remove{BPI 2012 Challenge dataset} \add{BPIC 2012} dataset, defined in \tablename~\ref{table:dataset}, also used in \cite{BurattinMS16}, and we slightly edited the queries in the same paper as illustrated in \tablename~\ref{table:burattin_model}, where all the models $M_i$ and $M_j$ with $i<j$ are always the former a subset of the latter, while $M_{i+1}$ provides a 5-fold increase from $M_i$. For Declare Analyzer, we chose to load the dataset via MapDB\footnote{\url{https://mapdb.org/}}, thus aligning the authors' hash map representations of the dataset with a representation similar to the one for relational databases. 


Our experiments indicate that, overall, we are 2-3 times orders of magnitude more performant than DeclareAnalyzer. The conjunctive query denoted as \texttt{KnoBAB+CQ} demonstrates greater performance that \texttt{KnoBAB+Support}, as the calculations required for the support values per clause are more costly for smaller models. Though this is only within the order of the milliseconds. For an increase in model size, Declare Analyzer has a much greater time increase than KnoBAB (the best case for Declare Analyzer is over an order of magnitude greater than that of KnoBAB). %Regarding scalability, the former exhibits better performance than KnoBAB, where the complexity increase is less than that of KnoBAB. 
While the linear interpolation of Declare Analyzer provides a slope of $3.47\cdot 10^2$ \textit{ms} per model size, KnoBAB provides a slope of $~10^1$, thus providing an inferior overall growth rate. To explain the abrupt time increase from $M_1$ to $M_2$, we encourage the reader to refer back to the query plan from \figurename~\ref{fig:knobab_pipeline} with reference to the model from \tablename~\ref{table:burattin_model}. With each increase in model size, entirely new event labels and data payloads are considered (albeit the conditions within each sub-model are similar). As KnoBAB thrives when data access can be limited, the addition of new data requires more decomposition within the atomization pipeline, and, as more atoms are now considered, querying will also suffer as more data is going to be accessed. As a result, the complexity increase is worse than examples tailored to benefit data access limiting as in the previous scenarios where queries were sharing multiple and frequent activity conditions. Still, Declare Analyzer will always completely scan all the events by design despite, for some queries, we might exclude scanning irrelevant trace events.

%Our experiments show that while our proposed solution has temporal fluctuations caused by the fact that the query was running on the order of the milliseconds, the running time for Declare Analyzer is constantly increasing with the number of the available queries. Overall, our solution outperforms by 2-3 orders of magnitude the one from Declare Analyzer.%


%The benefits from the custom query plan are most obvious in the process mining stage, where a log consisting of potentially thousands of traces is tested against all combinations of clauses. However, computational gains can also be evidenced when the same querying approach is adapted to a runtime scenario, where we are querying only 1 trace against an existing model (which requires much less computation as a whole).
%
%For $\mathcal{C}$ Declare clauses, where $\mathcal{N}$ is the data loading cost, implementations without a KB suffer, resulting in $\mathcal{O(C \cdot N)}$ complexity. With a KB, data loading is necessary only once, enhancing the complexity to $\mathcal{O(C + N)}$.
%
%However these are computationally bottlenecked to the efficiency of these systems themselves, regardless of the optimality of the conformance checking.
%
%\RevDel{SQL miner, due to the query structure, requires vast amounts of secondary memory for temporary caching of query computation, {much less than KnoBAB requires}.}
