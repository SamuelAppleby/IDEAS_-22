
\section{Experimental Analysis}\label{sec:exp}
These benchmarks were carried out on a Razer Blade Pro laptop: Intel Core i7-10875H CPU @ 2.30GHz - 5.10 GHz (8 Cores 16 Logical Processors), 16GB DDR4 2933MHz RAM, 180GB free disk space; running on Ubuntu 20.04.

\begin{figure}[!t]
	\centering
%<<<<<<< HEAD
	\includegraphics[width=.8\textwidth]{images/sqlminer_benchmark.pdf}
	\caption{KnoBAB vs SQLMiner Performance for 25  clauses with frequent activity labels with Support and Trace Information. OOM indicates Out of Primary Memory.}\label{fig:vsSQL}
%=======
%	\caption{KnoBAB vs SQLMiner Performance for Varying Model Size.}\label{fig:vsSQL}
%>>>>>>> 33357ef815c5a234b684fdafdceed45bf3a89be0
\end{figure}


\subsection{SQLMiner}\label{ssec:sqlmin}
These experiments want to test our working hypotheses for the possibility of engineering a tailored relational database architecture that can outperform process mining through conformance checking running on traditional relational databases. In the latter, no \LTLf operators are exploited and non-customary tabular representation is exploited. Given that the SQL provided in \cite{Schonig15,SchonigRCJM16} might only return the \textsc{Support} associated to each candidate declare clause (\texttt{SQLMiner+Support}), we provided the least possible changes to also associate to each candidate clause the set of all the traces satisfying it. This was achieved by extending the activation condition expressed in SQL using \texttt{array\_aggr} included in \textbf{PostgreSQL 14.2} to list such traces (\texttt{SQLMiner+TraceInfo}). For comparing the same settings in KnoBAB, we run both Max-SAT and \textsc{Support} queries with the difference that, in our case, both of these implementations will always return, per intermediate result specification, the trace information satisfying each possible model clause.
We exploited the same hospital log dataset from \cite{SchonigRCJM16}. To test the scalability of the solutions, we recorded the query's runtime with increasing log size: we randomly sampled the log with three sub-logs containing 10, 100 and 1000 traces, while guaranteeing that each sub-log is always a subset of the greater sub-logs. For each sub-log, we generated 8 distinct models (as benchmarked in \cite{Schonig15}). Each model consisted of 25 clauses instantiating the same declare template (\textit{elected template}) with different activation and target conditions. Those did not consider payload conditions, and were only considering the most frequent activity labels appearing in the sub-log. Models of a greater size than this caused an exponential increase in required secondary memory for SQLMiner (on the order of TB), justifying our approach for a sampled model. In their approach, each model was queried by running the SQL query corresponding to the \textit{elected template}, and the specific activation and target conditions from the model's clauses were specified as distinct rows in the \textsf{Action} table.

The outcome of such experiments is represented in \figurename~\ref{fig:vsSQL}, where each plot represents the running time associated to models containing the same \textit{elected template}. In the worst case scenario (\textsf{Response}), we exhibit similar query running times to SQLMiner. Even so, we are always providing trace information, and in case of \textsf{Response}, altering the SQL to provide this causes over an order of magnitude increase in complexity. In the best case scenario, we outperform SQLMiner by at most 5 orders of magnitude. This is because our query plan minimizes the access to the data queries and our computation avoided explicit computations of aggregations. This was achieved by sorting the intermediate results, and, as sorted results are guaranteed, performing counting operations via linear scans of the intermediate result representation. Our solution never exceeded the 16GB of primary memory while, for some more complex queries (top row of \figurename~\ref{fig:vsSQL}), SQLMiner exceeded it, thus proving that our solution is also memory efficient. One of the outstanding examples is \textsf{RespExistence}, where we are greatly more efficient that SQLMiner. This is a clear indicator of the potential gains from utilising our proposed \textsf{CountTable}, as no log scans are necessary. The original SQL query is required to scan the whole \texttt{Log} table (similar to our \texttt{ActivityTable}), which contained all of the trace events. We also remind the reader that the \textsf{CountTable} can be efficiently created while scanning the whole log dataset. This further validates that an adequate tabular representation twinned with \xLTLf operators extending the \LTLf specification for tabular data provides a suitable solution. Last, running time of the Max-SAT problem and \textsc{Support} for KnoBAB exhibited similar running times, while in PostgreSQL those exhibit huge variations depending on the query-plan rewriting performed by PostgreSQL query engine. For some elected templates, our proposed \texttt{SQLMiner+TraceInfo} formulation proved also to be more efficient than the \texttt{SQLMiner+Support} queries originally proposed by \cite{Schonig15} (which contain no trace information at all).



%\subsection{Declare Analyzer}\label{ssec:declan}

%<<<<<<< HEAD
%\begin{figure}[!t]
%	\centering
%	\includegraphics[width=.7\textwidth]{images/Buratto.pdf}
%	\begin{tabular}{l}
%		\toprule
%		\textit{Conjunctive Query}\\ 
%		\midrule
%		$q_1:= \DeclareClause{Response}{A\_SUBMITTED}{\textbf{true}}{A\_ACCEPTED}{\textbf{true}}$ \\
%		$q_2:= q_1\wedge \MonoDeclareClause{Exists}{\_\_trace\_payload}{\texttt{AMOUNT\_REQ}\geq 10^3}{\geq 1}$ \\
%		$q_3:=q_1\wedge \MonoDeclareClause{Exists}{\_\_trace\_payload}{\texttt{AMOUNT\_REQ}< 10^3}{\geq 1}$ \\
%		$q_4:=q_1\textrm{ where }\texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\
%		$q_5:=q_1\textrm{ where }\texttt{A\_SUBMITTED.org:resource}\neq\texttt{A\_ACCEPTED.org:resource}$ \\
%		\bottomrule
%	\end{tabular}
%	\caption{KnoBAB vs Declare Analyzed Performance.}\label{fig:vsBuatto}
%\end{figure} 
\subsection{Declare Analyzer}\label{ssec:declan}
The set of experiments on Declare Analyzer have the aim of comparing our proposed solution against a solution tailored for solving Declare Conjunctive Queries over logs running exclusively in primary memory. %In particular, we want to show the benefit of running a query plan when multiple sub-queries are shared: by increasing the size of the model by adding novel queries, we are expecting that while our solution exhibits a near constant running time, Declare Analyzer's running time should increase almost linearly as per the authors' claim.
%=======
\begin{table}[!t]
	\label{table:burattin_model}
	\centering
	\caption{Declare Models with their Respective Clauses}
	\resizebox{.7\textwidth}{!}{
		\begin{tabular}{ll}
			Model & Queries \\
			\toprule
			\multirow{5}{*}{$M_1=\qquad$} \rdelim\{{5}{3mm} & $q_{1}:= \DeclareClause{Response}{A\_SUBMITTED}{\textbf{true}}{A\_ACCEPTED}{\textbf{true}}$ \\ & $q_{2}:= \DeclareClause{Response}{A\_SUBMITTED}{\texttt{AMOUNT\_REQ} \geq 10^3}{A\_ACCEPTED}{\textbf{true}}$ \\&
			$q_{3}:= \DeclareClause{Response}{A\_SUBMITTED}{\texttt{AMOUNT\_REQ} < 10^3}{A\_ACCEPTED}{\textbf{true}}$ \\&
			$q_{4}:= q_{1} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\&
			$q_{5}:= q_{1} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}\neq\texttt{A\_ACCEPTED.org:resource}$ \\
			\toprule
			\multirow{5}{*}{$M_2=M_1+$}\rdelim\{{5}{3mm} & $q_{6}:= \DeclareClause{Response}{W\_Completeren aanvraag}{\textbf{true}}{W\_Valideren aanvraag}{\textbf{true}}$ \\ &
			$q_{7}:= \DeclareClause{Response}{W\_Completeren aanvraag}{\textbf{true}}{O\_CANCELLED}{\textbf{true}}$ \\&
			$q_{8}:= q_{6} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}\neq\texttt{A\_ACCEPTED.org:resource}$ \\&
			$q_{9}:= \DeclareClause{Response}{W\_Valideren aanvraag}{\texttt{AMOUNT\_REQ} = 5 \cdot 10^3}{O\_CANCELLED}{\textbf{true}} $ \\&
			$q_{10}:= q_{9} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\
			\toprule
			\multirow{5}{*}{$M_3=M_2+$}\rdelim\{{5}{3mm} & $q_{11}:= \DeclareClause{Response}{O\_SELECTED}{\textbf{true}}{O\_CANCELLED}{\textbf{true}}$ \\&
			$q_{12}:= q_{11} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\&
			$q_{13}:= \DeclareClause{Response}{O\_SELECTED}{\texttt{AMOUNT\_REQ} < 8 \cdot 10^3}{O\_CANCELLED}{\textbf{true}}$ \\&
			$q_{14}:= q_{13} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\&
			$q_{15}:= \DeclareClause{Response}{O\_SELECTED}{\texttt{AMOUNT\_REQ} > 10^3}{O\_CANCELLED}{\textbf{true}}$ \\
			\toprule
			\multirow{5}{*}{$M_4=M_3+$} \rdelim\{{5}{3mm}& $q_{16}:= \DeclareClause{Response}{A\_PARTLYSUBMITTED}{\textbf{true}}{A\_DECLINED}{\textbf{true}}$ \\&
			$q_{17}:= q_{16} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\&
			$q_{18}:= \DeclareClause{Response}{A\_PARTLYSUBMITTED}{\texttt{AMOUNT\_REQ} > 2 \cdot 10^4}{A\_DECLINED}{\textbf{true}}$ \\&
			$q_{19}:= \DeclareClause{Response}{A\_PARTLYSUBMITTED}{\texttt{AMOUNT\_REQ} > 2 \cdot 10^4}{A\_CANCELLED}{\textbf{true}}$ \\&
			$q_{20}:= q_{18} \textrm{ where } \texttt{A\_SUBMITTED.org:resource}=\texttt{A\_ACCEPTED.org:resource}$ \\
	\end{tabular}}
\end{table}
\begin{figure}[!t]
	\centering
	\includegraphics[width=.65\textwidth]{images/burattin_benchmark.pdf}
	\caption{KnoBAB vs Declare Analyzer Performance for Different Models. Knobab + CQ indicates the intersection, per trace, of all the satisfaction result representations i.e. returning  the traces that fulfilled all the clauses.}\label{fig:vsSQL}
\end{figure}

%>>>>>>> 33357ef815c5a234b684fdafdceed45bf3a89be0

In order to do so, we exploited the BPI 2012 Challenge dataset, also used in \cite{BurattinMS16}, and we slightly edited the queries in the same paper as illustrated in \figurename~\ref{fig:vsBuatto}. For Declare Analyzer, we chose to load the dataset via MapDB\footnote{\url{https://mapdb.org/}}, thus aligning the authors' hash map representations of the dataset with a representation similar to the one for relational databases. 

Our experiments indicate that, overall, we are 2-3 times orders of magnitude more performant than DeclareAnalyzer. \texttt{KnoBAB + CQ} demonstrates greater performance that \texttt{KnoBAB + Support}, as the calculations required for the support values per clause are more costly for smaller models. Though this is within only within the order of the milliseconds. For an increase in model size, Declare Analyzer has a much greater time increase than KnoBAB (the best case for Declare Analyzer is over an order of magnitude greater than that of KnoBAB). Regarding scalability, the former exhibits better performance than KnoBAB, where the complexity increase is less than that of KnoBAB. 

To explain this, we encourage the reader to refer back to the query plan from \figurename~\ref{fig:knobab_pipeline} with reference to the model from \tablename~\ref{table:burattin_model}. With each increase in model size, entirely new event labels and data payloads are considered (albeit the conditions within each sub-model are similar). As KnoBAB thrives when data access can be limited, the addition of new data requires more decomposition within the atomization pipeline, and, as more atoms are now considered, querying will also suffer. As a result, the complexity increase is worse than examples tailored to benefit data access limiting.

%Our experiments show that while our proposed solution has temporal fluctuations caused by the fact that the query was running on the order of the milliseconds, the running time for Declare Analyzer is constantly increasing with the number of the available queries. Overall, our solution outperforms by 2-3 orders of magnitude the one from Declare Analyzer.%


%The benefits from the custom query plan are most obvious in the process mining stage, where a log consisting of potentially thousands of traces is tested against all combinations of clauses. However, computational gains can also be evidenced when the same querying approach is adapted to a runtime scenario, where we are querying only 1 trace against an existing model (which requires much less computation as a whole).
%
%For $\mathcal{C}$ Declare clauses, where $\mathcal{N}$ is the data loading cost, implementations without a KB suffer, resulting in $\mathcal{O(C \cdot N)}$ complexity. With a KB, data loading is necessary only once, enhancing the complexity to $\mathcal{O(C + N)}$.
%
%However these are computationally bottlenecked to the efficiency of these systems themselves, regardless of the optimality of the conformance checking.
%
%\RevDel{SQL miner, due to the query structure, requires vast amounts of secondary memory for temporary caching of query computation, {much less than KnoBAB requires}.}
