\subsection{Query Compiler}\label{sec:qc}
The query compiler is structured into three main phases. \textit{(i)} The \textit{atomization pipeline}  rewrites the data predicates 
associated with each activity label as a 
disjunction of mutually exclusive data conditions. We can tune KnoBAB to always atomize each possible activity label if it exists any Declare Constraint associating it to a data condition as in \cite{bpm21}, or we can choose to provide such an interval decomposition only to the Declare constraints exhibiting data conditions. While the former approach will maximise the access to the \textsf{AttributeTable}s, the latter will maximise the access to the \textsf{ActTable}. By doing so, we can ensure that the data satisfying some given properties can be visited at most once, thus guaranteeing the assumptions from \cite{BellatrecheKB21} also at the data accessing level. Correlation conditions do not undergo this rewriting step. The atomized model in \figurename~\ref{fig:knobab_pipeline} replaces the non-correlation data predicates with the outcome of the atomization process as in \cite{bpm21}. 


We \textit{(ii)} rewrite each Declare constraint as a \xLTLf formula, where the activations (and the potential target) conditions are instantiated with either just activity labels or also with associated data conditions as per the previous atomization step. 
Each sub-expression appears at most once as in \cite{BellatrecheKB21} by representing every single node in the query plan at most once: this is ensured by an internal query manager cache. The resulting query plan considering the simultaneous execution of multiple queries can be represented as a \textsc{Direct Acyclic Graph} (DAG).  
For each declarative clause appearing more than once (e.g., $m>1$), the associated \xLTLf expression will be computed at most  once, while its resulting data is going to be accessed $m$ times by the final aggregator: as per \figurename~\ref{fig:knobab_pipeline}, despite \textsc{Response} might be considered a subquery of \textsc{Succession}, the Max-SAT is still going to retrieve the output provided by the associated sub-expression. Green arrows remark operators' output shared among operators. Please also observe that operators with the same name and arguments but marked either with activation, target, or no specification are considered different as they provide different results, and therefore are not merged together. 
%\textit{Second}, we rewrite each Declare constraint as a \xLTLf formula, where the activations (and the potential target) conditions are instantiated with either just activity labels or also with associated data conditions as per previous atomization step. This step is mediated through a configuration file loaded at warm-up time, where novel declare constraints can be expressed as \xLTLf formulae. We guarantee to represent each subquery at most once as in \cite{BellatrecheKB21} by representing each node in the query plan at most once: this is ensured by an internal cache. At the end of this process, the query plan considering the simultaneous execution of multiple queries can be represented as a \textsc{Direct Acyclic Graph} (DAG). 
%>>>>>>> main
%>>>>>>> b69bdd98c4f898e5f4801fc44cbae460ecd04aef
This includes distinctions between timed and untimed operators.

Given that our execution engine provides the possibility of running a query plan in either a parallel or a sequential mode, we need   an additional step. \textit{(iii)} The previous DAG  represents a dependency graph, where a link between an ancestor and one of its descendants implies that the latter has to be computed before the former, thus suggesting an execution order. \figurename~\ref{fig:knobab_pipeline} depicts this as an arrow starting from the ancestor. To enforce that, we perform a lexicographical order over the DAG, through which we compute the maximum depth level associated with each node of the graph. %After doing so, 
We then represent the query graph as a stack of depth levels, where each operator on it can be run in parallel alongside its siblings.
 This proves that the computation of Declare Clauses can be reduced into an embarrassingly parallel problem, as the layered execution guarantees that no thread communication needs to happen, and that multiple threads could access contemporary the partial results associated with the immediately-descendant operators, as the former will return all of the events where the condition happened, while the latter will just return the trace event satisfying such condition alongside the required activations/targets listed in $L$. Furthermore, the proposed parallelization ensures minimizing the data access for computing the query. The DAG \figurename~\ref{fig:knobab_pipeline} depicts a query plan.
