%\subsection{Data-Aware Conformance Checking}
%% Second paragraph: comparison with Burattin. 
\paragraph*{Data-Aware Conformance Checking}
%\begin{itemize}
%	\item{Briefly introduce what Burattin does}
%	\item{How this problem is related to ours?}
%	\item{How many clauses does Burattin consider at a time?}
%	\item{Still, can those be also different types of clauses or not?}
%	\item{On the other hand, do they have proper support for data conditions and event labels?}
%	\item{Do they use a Knowledge Base? If not, how are they querying?}
%	\item{What is their proposed parallelization approach? What is the difference and similarity with our approach?}
%\end{itemize}
%Previous solutions, while performing conformance checking, do not consider the addition of payload information for each event, limiting the expressiveness of the model. 
\texttt{Declare Analyzer}\footnote{\url{http://www.promtools.org/doku.php?id=prom611}} \cite{BurattinMS16} proposes one of the first solutions that provides conformance checking to data-aware logs. Declare templates are decomposed into \LTLf expressions, that not only contain event information, but a payload associated to each event per clause. This information manifests itself as \emph{activation} and \emph{correlation} conditions for to the \emph{activation} and \emph{target} events respectively. This approach is claimed to be is more optimised than the previous works \cite{VanDerAalst2005}.%, which do not even consider data conditions. 

While this solution exhibits good performance, it does not exploit the benefits that a relational database can provide, where techniques such as query optimisation can benefit computation. Due to this, they cannot capitalise on possible performance gains by running multiple queries so to minimize the data access as in \cite{BellatrecheKB21}. Another further gain that can be made from structuring queries in a query plan (that this approach does not support) is that it can be easily parallelised. In addition, it is inflexible; each clause is implemented from scratch, and, as they do not share the operators we provide, the addition of further clauses would require an entirely new implementation, while our solution support the definition of potential new Declare template via configuration files loaded in the initialization phase. We propose a more generalised solution, where each clause is composed of a combination of unique operators, allowing for any new clause to be included on the fly. 

While data conditions are considered, the proposed solution regards only one predicate per event (through target and correlation conditions). Therefore, multiple data conditions cannot be combined through either conjunction or disjunction, limiting the possible expressiveness of the conformance calculation. For example, queries such as 
$\DeclareClause{Response}{RainStart}{moistureContent\\>50\% \wedge consecutiveRain>3}{FloodCheck}{forecast=rain}$ 
%$\mathbf{Response(RainStart \{moistureContent>50\%  \& consecutiveRain>3\}, FloodCheck\{forecast=rain\})}$
are only possible by running a query per data condition and intersecting the returned set. KnoBAB provides the ability to combine several conditions per event, allowing queries like above to be processed. Furthermore, authors do not exploit efficient relational algebra operators when possible, as full-outer-theta-joins (or theta-joins) for unions (or conjunctions) with correlation conditions.


%\subsection{Process Mining through Conformance Checking}
\paragraph*{Process Mining through Conformance Checking}
%\begin{itemize}
%	\item{Briefly introduce what SQLMiner does}
%	\item{How this problem is related to ours}
%	\item{How many clauses does SQLMiner consider at a time?}
%	\item{Still, can those be also different types of clauses or not?}
%	\item{On the other hand, can we potentially do it? does our solution, on the other hand, provide a constraint on the type of the model, or is this general enough to consider each possible clause at the same time?}
%\end{itemize}
Some approaches utilise conformance checking as a mechanism to mine declarative \RevRepl{processes}{models}  from an event log \RevAdd{by testing the validity of each possible clause over each possible trace}. SQLMiner \cite{SchonigRCJM16} \RevRepl{uses traditional}{does so via} SQL queries \cite{Schonig15} \RevDel{as a to exercise this}\RevRepl{, E}{where e}ach specified declarative \RevRepl{constraint}{Template}, e.g. \emph{Response}, \RevRepl{has been}{is} converted into a SQL representation. \RevAdd{Then, the outcome of such a query is a declare clause distinct by different activation and target combination.} 

To achieve this, the event log is \RevAdd{first} loaded into the database as table \RevDel{, on which the queries are actioned}. \RevRepl{F}{Then, f}or every activation and target combination \RevRepl{a parametrized template of the the current clause is generated, and its conformance against the table}{of the declarative template, the authors store the candidate activation and target conditions to be tested in another relational table}. \RevRepl{To extend the functionality of each clause,}{For scoring the validity of each candidate clause}, the authors also calculate \emph{Support} and \emph{Confidence} values, to determine the precision and reliability of the calculation respectively. Records which do not pass pre-determined \emph{Support} and \emph{Confidence} thresholds are filtered from the data. \RevAdd{Despite } SQL also supports data constraints, \RevAdd{this process considers neither activation, nor target, nor correlation conditions}.

\RevAdd{Despite the authors } provide additional \RevAdd{data} perspectives through `Resource Assignment Constraints' clauses, distinct from the Declare ones, these additions were only considered as an additional perspective \RevRepl{as a whole}{to the global trace payload}, and do provide the functionality necessary for payload on an event\RevDel{, nor for Declare}. Furthermore, this data exists in isolation from the event information, and is only used as a filter on the data in the log table. KnoBAB provides payload information \RevAdd{both \emph{per trace}} and \emph{per event}, which could also be stored in a separate table as SQLMiner suggests, thus providing greater expressiveness per clause.

SQLMiner queries can be chained together, using \texttt{\textbf{SET UNION}}, though this provides no \RevAdd{possibility for testing which are the clauses that are satisfied by the majority of the traces.}, Furthermore, these query plans are not optimized as in \cite{BellatrecheKB21}, thus failing at minimizing the data access. This is inferior to KnoBAB, which has the ability to process multiple queries simultaneously. This allows for further optimisation of the query plan, which can exploit common sub-expression \emph{across queries}. This query plan can then be parallelized and processed with batch computation per query layer.
