\paragraph*{Data-Aware Conformance Checking}
\texttt{Declare Analyzer}\footnote{\url{http://www.promtools.org/doku.php?id=prom611}} \cite{BurattinMS16} proposes one of the latest solutions for conformance checking over data-aware logs. Declare templates are decomposed into \LTLf expressions (as per the last column of Table \ref{tab:dt}), that not only contain event information, but a payload associated to each event per clause. 
This approach is claimed to be  more optimised than prior works \cite{VanDerAalst2005}.  
Such solution
%While this solution exhibits good performance, it 
does not exploit RDBMS's 
%the 
benefits %that a relational database can provide, 
where %techniques such as 
query optimisations enhance query running times. 
So, no possible performance gains by shared sub-queries 
is considered so to minimize the data access, e.g., by conveniently 
structuring queries in a query plan \cite{BellatrecheKB21}. 
In addition, the authors
scan all of the traces completely for each Declare clause, while our proposed solution minimizes the data access by only accessing the data relevant for running the model-checking query. 
As their solution does not exploit multiple queries running processes, sub-queries or entire clauses appearing multiple times in the model might be recomputed multiple times, thus tampering with the overall running time.
As per their implementation of the \LTLf operators, authors do not exploit efficient relational algebra operators when possible, as full-outer-theta-joins (or theta-joins) for unions (or conjunctions) with correlation conditions.
%
Last, each clause is completely hardcoded and, as they do not support novel templates via the definition of novel \LTLf formulae, as we instead do. The addition of further Declare clauses would require an entirely new implementation. KnoBAB, on the other hand, supports the definition of potential new Declare templates via configuration files loaded at warm-up, %We propose a more generalised solution, where each clause is composed of a combination of unique operators, allowing for any new clause to be included on the fly.
thus enabling a more general result that goes beyond the Declare language and that can be applied to any temporal specification exploiting \LTLf.

A more recent approach \cite{abs-2112-04623} defined specific data structures for a limited support of declarative queries in sublinear-time. Still, this approach has the major shortcoming of pre-computing the possible \textsf{Precedence} or \textsf{Response} queries at loading time. This approach does not scale up for other possible declarative templates, as this might require to extend the data representation  with additional data structures. On the other hand, our proposed approach is query independent and supports all of the possible queries that might be expressed in \xLTLf. Furthermore, this approach supports logs with neither trace nor event payload, thus preventing from easily extending it with activation, target, and correlation conditions involving data predicates. As this approach had a limited query expressive power, it was not considered in our benchmarks.

\paragraph*{Process Mining through Conformance Checking}
Some approaches utilise conformance checking as a mechanism to mine declarative {models}  from an event log: a scoring function tests % by testing 
the validity of each possible clause over each possible trace. %via a scoring function. 
SQLMiner \cite{SchonigRCJM16} {does so via} SQL queries \cite{Schonig15} {where e}ach specified declarative {template} % e.g. \emph{Response}, 
{is} converted into a SQL query. E.g., given the SQL formulation for the \textsf{Response} template, the query returns a table \textsf{(Activation,Target,Score)} where each row $\braket{\textsf{a},\textsf{b},s}$ represents a candidate clause $\Sdeclare{Response}{a}{b}$, and $s$ is its score.
%{Then, the outcome of such a query is a declare clause distinct by different activation and target combination.} 

Each event log, as well as each activation and target activity label for generating the candidate Declare constraints to be tested, are stored in distinct relational tables. While the former are represented in  \textsf{Log(Id,Trace,ActivityId,Event)}, the latter are stored in \textsf{Actions(ActivationId,TargetId)}. The authors consider 
%To achieve this, the event log is {first} loaded into the database as table. {Then, f}or every activation and target combination {of the declarative template, the authors store the candidate activation and target conditions to be tested in another relational table}. {For scoring the validity of each candidate clause}, the authors also calculate 
\textsc{Support} and \textsc{Confidence} scoring functions to determine the precision and reliability of the calculation. Records which do not pass pre-determined \textsc{Support} and \textsc{Confidence} thresholds are filtered out from the data. While SQL also supports data constraints, this solution considers Declare clauses with neither activation, nor target, %, nor correlation conditions}.
%with payload conditions, as well as 
nor correlation ones with payload predicates. This problem is also shared with  more recent approaches where, despite SQL syntax is extended, no evidence of data predicates is given \cite{Polyvyanyy2022}. 

{Despite the authors} exploit %providing additional 
{data} perspectives in `Resource Assignment Constraints' clauses, distinct from the Declare ones, %these additions were only considered  {as 
	only trace payload conditions are considered. Instead, KnoBAB supports payload information and predicate testing {both \emph{per trace}} and \emph{per event}, which could also be stored in a separate table as SQLMiner suggests, thus providing greater expressiveness per clause.
%
SQLMiner queries can be chained together using \texttt{\textbf{SET UNION}}, though this provides no possibility for testing which are the clauses that are satisfied by the majority of the traces (Max-SAT). These query plans are not optimized as in \cite{BellatrecheKB21}, thus failing at both minimizing the data access
and running multiple shared sub-queries only once.
This is inferior to KnoBAB, which has the ability to process multiple 
declarative clauses from disparate templates. % in the same query plan.
%queries simultaneously. This allows for further optimisation of the query plan, which can exploit common sub-expression \emph{across queries}. This query plan can then be parallelized and processed with batch computation per query layer.
