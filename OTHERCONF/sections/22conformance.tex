\paragraph*{Data-Aware Conformance Checking}
\texttt{Declare Analyzer}\footnote{\url{http://www.promtools.org/doku.php?id=prom611}} \cite{BurattinMS16} proposes one of the latest solutions for conformance checking over data-aware logs. Declare templates are decomposed into \LTLf expressions (as per the last column of Table \ref{tab:dt}), that not only contain event information, but a payload associated to each event per clause. %This information manifests itself as \emph{activation} and \emph{correlation} conditions for to the \emph{activation} and \emph{target} events respectively. 
This approach is claimed to be  more optimised than the previous works \cite{VanDerAalst2005}.%, which do not even consider data conditions. 

While this solution exhibits good performance, it does not exploit the benefits that a relational database can provide, where techniques such as query optimisation can benefit from an optimised query plan. Due to this, they cannot capitalise on possible performance gains by shared sub-queries %at most once 
so to minimize the data access, e.g., by conveniently % as in . Another further gain that can be made from 
structuring queries in a query plan \cite{BellatrecheKB21} %(that this approach does not support) is that it can be easily parallelised. 
In addition, the authors' proposed (yet unimplemented) parallelization is rather inflexible: by only parallelising on the execution of each single statement, we still risk to re-compute the same sub-queries multiple times, thus tampering the potential speed-up gain.

While data conditions are considered, the authors regard only one predicate per event in their experiments, where no logical connectors are considered. Therefore, multiple data conditions cannot be combined through either conjunction or disjunction, limiting the possible expressiveness of the conformance calculation. For example, queries such as 
$\DeclareClause{Response}{RainStart}{\\moistureContent>50\% \wedge consecutiveRain>3}{FloodCheck}{forecast=rain}$ \pdfcomment[author=Giacomo]{Always use the same running example use case example for the Declare Clause}
%$\mathbf{Response(RainStart \{moistureContent>50\%  \& consecutiveRain>3\}, FloodCheck\{forecast=rain\})}$
are only possible by running a query per data condition and intersecting the returned set. KnoBAB provides the ability to combine several conditions per event, allowing queries like above to be processed. As per their implementation of the \LTLf operators, authors do not exploit efficient relational algebra operators when possible, as full-outer-theta-joins (or theta-joins) for unions (or conjunctions) with correlation conditions.

Last, each clause is completely hardcoded and, as they do not share the operators as we do, the addition of further Declare clauses would require an entirely new implementation. KnoBAB, on the other hand, supports the definition of potential new declare templates via configuration files loaded at warm-up. %We propose a more generalised solution, where each clause is composed of a combination of unique operators, allowing for any new clause to be included on the fly.




\paragraph*{Process Mining through Conformance Checking}
Some approaches utilise conformance checking as a mechanism to mine declarative {models}  from an event log: a scoring function tests % by testing 
the validity of each possible clause over each possible trace. %via a scoring function. 
SQLMiner \cite{SchonigRCJM16} {does so via} SQL queries \cite{Schonig15} {where e}ach specified declarative {template} % e.g. \emph{Response}, 
{is} converted into a SQL query. E.g., given the SQL formulation for the \textsf{Response} template, the query returns a table \textsf{(Activation,Target,Score)} representing a candidate clause $\Sdeclare{Response}{a}{b}$ for each row $\braket{\textsf{a},\textsf{b},s}$ in it, where $s$ is   score associated to the candidate clause.
%{Then, the outcome of such a query is a declare clause distinct by different activation and target combination.} 

Each event log, as well as each activation and target activity label for generating the candidate Declare constraints to be tested, are stored in distinct relational tables. While the former are represented in  \textsf{Log(Id,TraceId,ActivityLabel,EventId)}, the latter are stored in \textsf{Actions(ActivationLabel,TargetLabel)}. The authors consider 
%To achieve this, the event log is {first} loaded into the database as table. {Then, f}or every activation and target combination {of the declarative template, the authors store the candidate activation and target conditions to be tested in another relational table}. {For scoring the validity of each candidate clause}, the authors also calculate 
\textsc{Support} and \textsc{Confidence} scoring functions to determine the precision and reliability of the calculation respectively. Records which do not pass pre-determined \textsc{Support} and \textsc{Confidence} thresholds are filtered from the data. While SQL also supports data constraints, this solution considers Declare clauses with neither activation, nor target %, nor correlation conditions}.
with payload conditions, as well as no correlation ones.

{Despite the authors} providing additional {data} perspectives through `Resource Assignment Constraints' clauses, distinct from the Declare ones, these additions were only considered  {as trace payload}. On the other hand, KnoBAB provides payload information {both \emph{per trace}} and \emph{per event}, which could also be stored in a separate table as SQLMiner suggests, thus providing greater expressiveness per clause.

SQLMiner queries can be chained together, using \texttt{\textbf{SET UNION}}, though this provides no possibility for testing which are the clauses that are satisfied by the majority of the traces (Max-SAT). These query plans are not optimized as in \cite{BellatrecheKB21}, thus failing at both minimizing the data access
and running multiple shared sub-queries only once.
This is inferior to KnoBAB, which has the ability to process multiple 
declarative clauses from disparate templates in the same query plan.
%queries simultaneously. This allows for further optimisation of the query plan, which can exploit common sub-expression \emph{across queries}. This query plan can then be parallelized and processed with batch computation per query layer.
