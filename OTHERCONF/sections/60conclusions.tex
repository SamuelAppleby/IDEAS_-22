\section{Conclusions and Future Works}
We propose KnoBAB, a full relational database architecture for computing Conformance Checking queries (conjunctive query), as well Max-SAT and clause support functions.  KnoBAB consists of data loader, query compiler, and execution engine, thus fully matching an architecture of a relational database. This solution was enabled by the extension of the traditional \LTLf operators, providing a algebraic semantics to declarative temporal models, to support data operations over tuple representations. Based on the latest solutions on current database literature, the query plan was also designed to minimize the data access by running the common sub-queries at most once.

KnoBAB outperforms state of the art solutions both tailored to the specific dataset or based on traditional relational databases running SQL queries.  This solution will enable us to learn models exploiting abductive reasoning rather than traditional mining techniques, thus also providing safety guarantees and models that are inconsistency free. Such solutions will be also extremely useful when dealing with noisy data \cite{PicadoDTL20}.

Future works will provide extensive benchmarks for bigger log datasets and will provide speed-up results for the parallelized execution of the resulting query plan: despite this was already implemented, we postpone those results due to the lack of space on the present paper. For the time being, the logs available from the research community are quite compact, and therefore the whole dataset is well fit in primary memory. Dealing with actual big data solution will require us to migrate the data store location to secondary memory, thus requiring the adoption of Near-Data Processing techniques \cite{GuYBJLYKKYCJC16}. 

Last, we are going to extend the expressivity of the relational comparisons by including the support of mathematical expressions, thus allowing to faithfully representing temporal data aspects as in \cite{BurattinMS16}. Future works will also assess the feasibility of extending the proposed solution to repair traces with the minimum amount of edits required to make the trace compliant to the queried model. This will provide a further extension of the state of the art, as current trace repair solutions do not support correlation conditions \cite{bpm21}.

%\textit{Summarize the abstract even more, as now you need also to summarize the outcome of the experiments. Furthermore, say what was legitimately left out, and which are the future works being scheduled for extending the present work.}
%
%In particular:
%\begin{itemize}
%	\item Are all of the open questions in the introduction closed at this point? Are all the questions answered? 
%	\item If something relevant is left out, is that considered for future work?
%\end{itemize}
%
%Experiments on parallelizations are omitted due to lack of space. Are implemented, but they are going to be tested in our future work.
%
%Talk about the support of runtime traces (require much less computation than data mining - many fewer clauses, much less data (only 1 trace))
%
%Another possible addition could be the inclusion of external data conditions that are not bound to data payloads. We currently consider target, correlation and theta conditions, but not external factors. 
%
%Therefore, we can represent: 
%
%\emph{`If it has started raining, and the moisture content of the soil is above 50 percent, a flood check must happen if more rain is forecast'}.  
%As a Declare clause, this would formulate:
%
%{\tiny$\mathbf{Response(RainStart \{moistureContent>50\%\}, FloodCheck\{forecast=rain\}) WHERE RainStart\{location\} = FloodCheck\{location\}}$}
%
%What KnoBAB does not currently support, however, is the addition of an external global condition. 
%
%Therefore a constraint such as:
%
%\emph{`If it has started raining, and the moisture content of the soil is above 50 percent, a flood check must happen \textbf{2 days after} if more rain is forecast'}.  
%
%Is not currently supported. The inclusion of these conditions would allow for greater expressiveness of each clause, and therefore the declarative model itself.