\section{Introduction}

\paragraph*{Brief introduction of your general area of interest: provide the \textbf{context} to the overall general setting.} \textit{Conformance checking} is an integral part of artificial intelligence bridging data mining and business process management \cite{bpm21}. It assesses whether a sequence of distinguishable events (i.e., a \textit{trace}) conforms to the expected process behaviour represented as a \textit{process model} \cite{RozinatA08}.These constraints can be expressed by either multiple declarative human readable templates in conjunction \cite{Li2020}, where each one of these represent a different temporal behavioural pattern, or equivalently as Finite State Machines \cite{AgostinelliBFMM21}. %Such a model might be either represented as a set of temporal clauses, determining correlations between events happening at a previous time of the trace (\textit{activation}) and others happening in the immediate future (\textit{target}).
Despite the two approaches are equivalent, in this paper we are going to focus on the former representation, as the latter provide to be quite inefficient for performing conformance checking of data-aware models \cite{bpm21}. Such declarative temporal rules are not limited to the mere presence of specific events within the trace, but also determine temporal occurrence patterns. Such models are usually exploited as the core behind an AI's temporal decision making. Therefore, a model defines an AI's unique behaviour, and by varying the set of expressions within these models, a entirely new set of responses can be generated.  Mining a hospital log consisting of successful traces (correct procedures, adequate medication etc.) could produce a model representing the patterns leading to `good' outcomes  \cite{Amantea2020}. Such mining approach could either retrieve either declarative models or sequential patterns of interest \cite{mining}. An AI could then query the resulting mined model for determining which clinical situation is likely to adhere to the expected clinical standards.

When a trace does not adhere to the model, we say that the trace is \textit{deviant} \cite{bpm21}. Given a Declare template \textsf{RespondedExistence}, \DeclareClause{RespondedExistence}{A}{\textbf{true}}{B}{\textbf{true}} is the instantiated Declare clause for event labels \texttt{A} and \texttt{B} that states \emph{`If event A happens, event B must happen sometime in the future'}: where A (B) is the \textit{activation} (\textit{target}) condition; a trace would be a deviant \emph{iff.} a trace contained an instance of A that was not followed by an instance of B. Furthermore, \textbf{true} predicates associated to activation (target) conditions can be enriched to express data conditions (\S\ref{sec:DAD}). As any declarative language, the specification of such templates can be expressed as a set of logical operators: in this field, Finite Liniear Time Logic (\LTLf) is usually exploited: for instance $\DeclareClause{RespondedExistence}{A}{$p$}{B}{$q$}$ becomes $\Future(A\wedge p) \Rightarrow \Future(B\wedge q)$, where $\Future(A\wedge p)$ express the intention that a trace labelled as $A$ satisfying a data condition $p$ should occur any time within the trace (\textit{ibid}.)

%%{\color{red}[TODO: replace] To further decompose these clauses, algebraic notation can be used to represent the set of operators that are the constituents of a given clause. Declare templates can be represented using \LTLf \cite{Li2020}. This allows for a flexible conformance checking implementation, as each clause can be represented as a unique pairing of \LTLf operators and join operators, for insatnce $\mathbf{RespondedExistence(A,B)}$ becomes $\Future(A) \Rightarrow \Future(B)$\PopUpComment{Giacomo}{Please observe that mathcal should be used only by surrounding the symbol of interest. Otherwise, in some other scenarios, you might have faults. Still, we are going to use a box/diamond notation for this paper. I added some macros for that}. As part of the process mining pipeline, conformance checking is used to identify patterns emerging from a given log. Therefore, process mining can actually be reduced to a conformance checking problem.}\PopUpComment{Giacomo}{Some of the contents in here are good, and should be put elsewhere in the introduction. "In fact, despite these operators might be applied to query plans similarly to relational algebra operators, no work -- to the best of our knowledge -- exploited this possibility". But, this should be linked to another kind of problem, too!}
\medskip

\paragraph*{Who might be interested in our solution? How these people might use this work?} \textit{Please provide the pieces of information that are specific to your own research field, and provide some use case examples motivating the practicality of your approach}  

Conformance checking can be applied to several different domains. First, given that process model information can be exploited to represent the tasks performed by both physical and cybernetic agents \cite{Ioanna}, this information can be exploited to detect \textbf{cyber-security attacks}, where a model can be extracted from previous invasions allowing common patterns to be identified \cite{BENASHER201551,LagraaS20}. Then, the conformity of any trace to the model might be exploited for determining whether an attack occurred or not. Second, temporal models extracted from hospital logs, consisting of diagnoses and treatments with their respective outcomes, could aid \textbf{healthcare} professionals in \textbf{decision making} \cite{Amantea2020}. In fact, such models allow to associate a precondition to a consequence within a specific clinical event of interest, thus providing whitebox explainable AI \cite{mining,KusumaKMHGJ20}. As a result of the process discovery, conformance checking tools might be exploited to assess whether the specific clinical case abides to the declarative rules associated to the model, thus allowing the prediction of a specific clinical event of interest. Last, most recent videogames also exploit AI features: existing state of the art already exploit automata \cite{Miyake2017} for modelling \textsc{Non Player Character}'s behaviours. As Declarative models and automata are completely equivalent approaches, developers might exploit the former to more compactly represent the latter. Furthermore, as debugging AI in videogames is a crucial challenge \cite{john2019debugging}, conformance checking solutions might be exploited for debugging unexpected behaviours in videogames. Furthermore, as AAA videogames already allow to track and log both players and NPC actions\footnote{\url{https://battlefieldtracker.com/}}, it might be also possible to use game logs for distinguishing winning strategies from losing ones \cite{mining}. As a result, analysis of an ongoing trace at runtime would then allow the model to `suggest' an action that is beneficial to the player based on the current state of the game, with the current strategy they are pursuing.

%\begin{itemize}
	%\item A cyber-security attack, where a model can be extracted from previous invasions allowing common patterns to be identified. Some solutions such as \cite{BENASHER201551} use technologies to identify this, but they do not use conformance checking. 
	%\item Suggesting actions to players in video games. Conformance checking applications for AI in video games has not previously been research; existing state of the art \cite{Miyake2017} use either automata or machine learning. Process mining would allow models to be extracted that represent unique strategies players have attempted in the past. Information regarding their `success' can also be stored. As a result, analysis of an ongoing trace at runtime would then allow the model to `suggest' an action that is beneficial to the player based on the current state of the game, with the current strategy they are pursuing.
%\end{itemize}




\paragraph*{What do I want to say (to the research community), precisely.} \textit{I want to communicate the general problem that I am aiming to solve} 
Current state of the art conformance checking solutions do not exploit the benefits of storing data in a custom relational database. When running queries, the same data is often accessed multiple times \cite{BurattinMS16,bpm21}. This is especially the case in the process of data-mining with large workloads \cite{SchonigRCJM16}, where the identification of patterns often share similar subqueries. On the other hand, existing solutions \cite{BellatrecheKB21} identify common sub-expressions within several queries running contemporarily, therefore reduce both the data access and the computation overhead to a minimum. This can be easily related to the conformance checking problem, where multiple declarative clauses from the same model might be assessed contemporarily. By decomposing these queries into \LTLf, a similar approach can also be followed. We propose that the queries, decomposed into \LTLf operators, can also follow a query plan similar to \cite{BellatrecheKB21}. We extend the approach by adapting the query plan to express \LTLf operators similarly to relational algebra operations, where common sub-expressions can still be rationalised. Still, there is some prior work on decomposing clauses into traditional SQL queries \cite{SchonigRCJM16}. This solution \emph{does} exploits the benefits of using a relational database (and therefore query plans) by transforming declare clauses into traditional SQL queries. However, this solution is limited as it neither considers data conditions (only event identifiers), nor considers multiple clauses pertaining to disparate Declare templates. This provides less functionality than we propose, where we are data-aware and theta conditions can be taken into account when performing any operators.

\paragraph*{Why do I want to talk about this problem? Why is it relevant?} \textit{Because current literature is lacking of a given aspect} 

%Conformance checking is an integral part of artificial intelligence that bridges data mining and business process management. 


Conformance checking can be extremely computationally intensive, both in time and storage, so optimised solutions are necessary to ensure a well performant implementation. To our knowledge, no solution existing whereby a relational base exploits optimised query plans, adapting solutions such as \cite{BellatrecheKB21}, in a business process environment using LTLf.
\medskip


\paragraph*{Now, communicate our idea also to the people working in our same area!} \textit{In particular, this means that we can go down in technicalities on what we want to solve, which are the primarily goals of our research, and which are the intermediate requirements/results leading to the results that we expect.} 

As process mining can be reduced to a conformance checking problem, a given log can be queried against a declarative model at runtime, and the same conformance checking calculations can be applied to generate its conformance \emph{at the current time}. Therefore, KnoBAB provides an optimized representation of the trace logs over which the declarative models $\mathcal{M}$ are going to be both queried and mined with LTLf.

We propose a knowledge base, KnoBAB, which provides efficient conformance checking by adapting query plan optimisations \cite{BellatrecheKB21} to LTLf. In addition, we provide data-aware capabilities, which discussed database solutions do not. KnoBAB provides the conformance of a \emph{trace} to a set of clauses, not the conformance of a clause against a log. This is more valuable in scenarios where trace information could point to where, and why, it was a deviant. Such knowledge could then be used for many features, such as generating the repair for this trace.
\medskip
 

The greatest amount of performance gain is due to the custom query plan, structured in such a way that multiple queries are stored within a graph, and then batch jobs are run using \textbf{parallelisation}. When process mining, large numbers of queries are performed, therefore there will be many instances of duplicate data accessing, resulting in poor optimisation. In this approach, there is the guarantee that unique data elements are obtained and processed only once, while current state of the art process-mining approaches access data per query. 
