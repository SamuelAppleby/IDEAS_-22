\section{Introduction}

\paragraph*{Brief introduction of your general area of interest: provide the \textbf{context} to the overall general setting.}
\textit{Conformance checking} is a well-known \textsc{Process Mining} technique determining if a sequence of distinguishable events (i.e., a \textit{trace}) conforms to the expected process behaviour represented as a \textit{process model} \cite{RozinatA08}. Such a model might be either represented as a set of temporal clauses, determining correlations between events happening at a previous time of the trace (\textit{activation}) and others happening in the immediate future (\textit{target}). Such temporal rules are not limited to the mere presence of specific events within the trace, but also determine specific patterns how such clauses might occur (\S\ref{sec:DAD}). When a trace does not adhere to the model, we say that the trace is \textit{deviant} \cite{bpm21}.
\medskip

\paragraph*{What do I want to say (to the research community), precisely.} \textit{I want to communicate the general problem that I am aiming to solve} Current state of the art conformance checking solutions do not exploit the benefits of storing data in a custom relational database. Some provide solutions that utilise existing database management systems, however these are computationally bottlenecked to the efficiency of these systems themselves, regardless of the optimality of the conformance checking. Therefore, We propose an optimised relational database, KnoBAB, through which query plans have been carefully implemented to exploit the problems arising from data mining specifically. KnoBAB also provides the conformance of a \emph{trace} to a set of clauses, not the conformance of a clause against a log. This is more valuable in scenarios where trace information could point to where, and why, it was a deviant. Such knowledge could then be used to generate the repair for this trace, for example.
\medskip

\paragraph*{Why do I want to talk about this problem? Why is it relevant?} \textit{Because current literature is lacking of a given aspect} Conformance checking is an integral part of artificial intelligence that bridges data mining and business process management. This process can be extremely computationally intensive, both in time and storage, so optimised solutions are necessary to ensure a well performant implementation. To our knowledge, no solution existing whereby a knowledge base exploits query plans in such a way or the benefit of conformance checking. 
\medskip

\paragraph*{Who might be interested in our solution? How these people might use this work?} \textit{Please provide the pieces of information that are specific to your own research field, and provide some use case examples motivating the practicality of your approach} KnoBAB has been implemented from a data mining approach. The methodology behind its design has been an abstract one, with the only bespoke characteristics being its ability to generate conformances efficiently. With any event log, across a multitude of formats (xes, .txt, tab), common patterns can be identified and extracted. One such example could be a cyber-security attack, where clauses can be extracted from previous attacks allowing identification of common patterns from particular types of invasion. Another example could be a hospital log consisting of diagnoses and treatments with their respective outcomes. Mining particular patterns within this data could aid healthcare professionals in future decision making. The benefits from the custom query plan are most obvious in the process mining stage, where a log consisting of potentially thousands of traces is tested against all combinations of clauses. However, computational gains can also be evidenced when the same querying approach is adapted to a runtime scenario, where we are querying only 1 trace against an existing model (which requires much less computation as a whole). 
\medskip


\paragraph*{Now, communicate our idea also to the people working in our same area!} \textit{In particular, this means that we can go down in technicalities on what we want to solve, which are the primarily goals of our research, and which are the intermediate requirements/results leading to the results that we expect.} 
In this paper, we propose KnoBAB to provide an optimized representation of the trace logs over which the declarative models $\mathcal{M}$ are going to be queried. The greatest amount of performance gain is due to the custom query plan. It is structured in such a way that multiple queries are stored within a graph, and then batch jobs are run using \textbf{parallelisation}. In this approach, there is the guarantee that unique data elements are obtained and processed only once. For current state of the art process-mining approaches, data is accessed per query. For large number of queries (which would be found when process mining), this is inefficient as duplication of data processing will occur. Shared query plan \cite{BellatrecheKB21}
